# gs-tryouts


GRIDSIGHT logo
GRIDSIGHT
Customer Data Engineer (Graduate/Entry-Level)
Sydney, New South Wales, Australia Â· 2 weeks ago Â· Over 100 people clicked apply

Responses managed off LinkedIn
How your profile and resume fit this job

Get AI-powered advice on this job and more exclusive features with Premium. Reactivate Premium

About the job

Gridsight is a rapidly growing Grid/CleanTech startup on a mission to accelerate global electrification and decarbonisation. We are building a vertical SaaS platform for electricity utilities, enabling them to modernise grid operations and unlock transformational flexibility capabilities such as dynamic operating envelopes and flexible interconnections. Having recently raised our Series A funding from Airtree Ventures, Energy Transition Ventures and Area VC, we are poised for rapid growth and are seeking talented individuals to join us on our mission.

As a Graduate Data Engineer at Gridsight, youâ€™ll engage your passion for distributed renewables with your experience leading and owning data pipelines to enhance and scale our platform to hundreds of thousands of meters across the globe. In this opportunity to design, develop, and optimise data pipelines and architectures for electricity distribution networks, youâ€™ll play a critical role in redefining and revolutionising the way utilities can enable the decentralisation and decarbonisation of the grid. This is an entry-level position open to 2024/2025 graduates. 

Key Responsibilities

    Create scalable and efficient production-quality ETL pipelines to handle large volumes of data from various sources.
    Integrate and transform a variety of meter and GIS data sources into common Gridsight schemas.
    Manage the end-to-end execution of customer data pipelines and remediate any failures.
    Ensure data accuracy, consistency, and integrity by implementing robust data validation and governance practices; monitor and optimise data pipelines and queries to ensure high performance and low latency.
    Work closely with data scientists, customer success engineers, and other key stakeholders to understand ongoing data needs; provide solutions that support their requirements.

Qualifications

    Bachelorâ€™s degree in Computer Science, Engineering, Mathematics, or a related discipline.
    Experience with SQL relational databases; exposure to data engineering tools such as dbt preferred.
    Hands-on experience building production-quality ETL pipelines.
    Proficiency with big data frameworks (Spark preferred).
    Familiarity with at least one major cloud computing provider (AWS preferred).
    Fluency in Python, the command line, and Git.
    Previous experience as a Data Engineering intern, preferably with delivering projects to external stakeholders or clients.
    Strong analytical and problem-solving skills, with a proactive attitude towards identifying and resolving technical challenges.
    Self-starter mentality; youâ€™re able to independently prioritise tasks and manage time effectively with minimal oversight.
    Excellent communication skills and ability to collaborate effectively within a start-up environment.

What We Offer

    Join a rapidly scaling venture-backed company on the first floor.
    Highly competitive salary and equity package.
    Flexible, hybrid working environment with a high performing, mission-driven team.



Current engineers:

### 1

A Mathematician turned Data Engineer; I have years of experience in many different BI projects for a wide variety of different industries, including the energy, asset management, financial services, and health sectors. 

My skills range from early data modelling (ERD and data mapping), transformations and cleaning for data analytics to diving into deep learning machine learning models. My experience has also equipped me with extensive stakeholder management skills.

Specialisations: Data transformations, Data modelling (ERD and Data Mapping), SQL, dbt, Databricks, Excel, Python, Azure ML, Dataiku DSS, Jupyter notebooks

BI Tools: Qlik, Power BI

### 2
Experienced analytics professional with a demonstrated history of working across multiple sectors, and a passion for deriving value from data.
Skilled in building and deploying data products, data modelling, simulation, building ML models, exploratory analysis, and data visualisation.


# New

Devops
Engineer â€“ Backend, Senior (Optional Remote)
Preferable Location(s): Sydney, Australia
Work Type: Full Time
The DevOps Team at Enosi is responsible for the development of Enosiâ€™s Powertracer Platform. At Enosi, the developers also own the operational support of the platform.



We have rewritten Powertracer to scale to over a hundred thousand households, businesses and generation sources, trading their energy and delivering traceability of supply to customers. At this scale we will be responsible for delivering the trading and tracing between supply and demand of electrical energy in the gigawatt hours a year range.



We are looking to grow out the DevOps team and and youâ€™ll have an opportunity to work across a codebase encompassing Go, Ruby, Javascript and Python with Terraform to build infrastructure, and a mixture of containers and serverless applications. In this role, you'll be expected to take ownership of technical projects that are critical to the success of the Powertracer.



If you are ready to challenge an industry that has long lagged behind the innovation curve, and be part of driving emissions to true zero, we want to hear from you.



Examples of problems you'll be solving


Simplifying the complexity in electricity purchasing

Optimising data pipelines and storage

Designing and implementing large, distributed calculation engines

Shaping our APIs, both internal and external, made up of a combination of RESTful and GraphQL endpoints

Implementing highly available time-series historian services, supporting both batch processing and near-real-time data streams

Implementing trading registries with immutable storage

Integrating blockchain/distributed ledger technologies to deliver increased trust in trading outcomes

Orchestrating monitoring to ensure operational support (hint: itâ€™s in our name â€“ we own the ops space too)

Contributing to the development process through code reviews, stand-ups, planning and retrospectives

Developing your solution incrementally, seeking reviews, and deploying often

Making use of automated processes (where appropriate), including the use of CI/CD ( ðŸ‘‹ Buildkite).

Working with a wide range of technologies, including AWS, Docker, Terraform and more!



You are great because â€¦



You have more than 5+ years software engineering experience

You have professional experience delivering engineering solutions Go

You are experienced in identifying best practices with AWS cloud technologies, including serverless and containerised services

You are happy to mentor those around you, helping raise everyoneâ€™s skills up

You know your way around both relational and NoSQL databases

You understand that the code and documentation you write is not only to meet the current requirements, but also to make it understandable for the next person who works on it

You have a strong sense of product and empathy for the human who is using it

You care about fostering a positive and collaborative environment

You are able to work independently on a project while effectively explaining your decisions and documenting them

You are always learning and excited to grow



Bonus points



If you are comfortable working with Node (for example, React)

If you've been part of a team that works on large-scale distributed systems

If youâ€™ve been working on infrastructure using Terraform

If you have exposure and understanding of distributed ledger technologies and cryptographic methods



You can show us your technical experience



We will want to understand more about your skills, including your engineering track record, experience across the software development lifecycle, agile methods, devops, incorporation of testing. Having some example work that could be reviewed would be appreciated. 

